{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ml-data)=\n",
    "# Data for Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this chapter, we're going to look at some issues around data for machine learning, and preparing it. This chapter is enormously indebted to the [**scikit-learn**](https://scikit-learn.org/) documentation, Chris Albon's [Machine Learning Flashcards](https://machinelearningflashcards.com/), and, an absolute classic, The Elements of Statistical Learning {cite:p}`hastie2009elements`.\n",
    "\n",
    "Time and time again, the reliability of ML models deployed in the real-world depends on the quality of their training data or its preparation, and the quality of your model is typically limited by the quality of your training data. This is less true for tabular data, where the value-add of data prep tends to be lower compared to, say, text or images—but it's still true to some extent.\n",
    "\n",
    "In this chapter, we're going to tackle a few different angles on data including transforming it to be scale free, imputing missing values, thinking about *feature engineering*, and working with pipelines. Note that we will only be scratching the surface of this very deep and fast-changing topic here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random_state = 42  # We'll use this throughout to make this page reproducible\n",
    "prng = np.random.default_rng(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use(\n",
    "    \"https://github.com/aeturrell/coding-for-economists/raw/main/plot_style.txt\"\n",
    ")\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n",
    "\n",
    "# Set max rows displayed for readability\n",
    "pd.set_option(\"display.max_rows\", 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "\n",
    "The context for this section is that some machine learning algorithms are not *scale-free*, ie what units your measurements in really matters and you will get better or worse results depending on whether you have rescaled your data appropriately. One algorithm that benefits from this  is the Support Vector Machine. Scaling and pre-processing can help in different ways, but one key way is by easing convergence (such as with non-penalised logistic regression).\n",
    "\n",
    "In this section, we'll also talk about some pitfalls with pre-processing—namely the risk of information leakage.\n",
    "\n",
    "Of course, machine learning isn't the only context in which you may want to pre-process your data by rescaling it somehow, and these methods can be used in other scenarios too.\n",
    "\n",
    "Note that while we focus on the transformations available within **scikit-learn**, there are many other packages that provide this functionality. [**feature-engine**](https://feature-engine.trainindata.com/) deserves a special mention."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation and data leakage\n",
    "\n",
    "Standardisation is a common requirement for many machine learning estimators. These estimators might not be able to work at peak performance if the individual features do not more or less look like standard, normally distributed data: that is, a Gaussian with zero mean and unit variance.\n",
    "\n",
    "In practice, we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation. ie\n",
    "\n",
    "$$\n",
    "x_t \\longrightarrow \\frac{x_t - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "**scikit-learn** provides tools for standardisation. We'll demonstrate, first creating some fake data with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [8, 7, 9, 11, 12, 13, 15, 5, 20, 0, 0.43, 16.7],\n",
    "        [0.1, 0.2, 0.3, 0.6, 0.7, 0.8, 0.9, 0.3, 0.7, 0.88, 0.33, 0.22],\n",
    "    ]\n",
    ").T\n",
    "print(\"The mean of X is:\")\n",
    "print(X.mean(axis=0).round(3))\n",
    "print(\"The std of X is:\")\n",
    "print(X.std(axis=0).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, everything is an object! We've created a scaler object. It has state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaler.mean_)\n",
    "print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use it as a *function* to scale other data. Well, technically, we're using the `transform()` *method*, which is available to scaler objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.transform(X)\n",
    "print(\"The mean of X is:\")\n",
    "print(X_scaled.mean(axis=0).round(3))\n",
    "print(\"The std of X is:\")\n",
    "print(X_scaled.std(axis=0).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we come to an important point: *your scaler should only be created from your training data*. Why? Because mean and standard deviations are *global functions* that take information from the *entire* series. Their definitions are:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{T} \\displaystyle\\sum_t x_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{\\displaystyle\\sum_t(x_t - \\mu)^2}{T}}\n",
    "$$\n",
    "\n",
    "And series get transformed as $x_t \\longrightarrow \\frac{x_t-\\mu}{\\sigma}$.\n",
    "\n",
    "So, if you naively use the mean and std from the entire series you are letting information from the test set into your scaling function, and this could enable (erroneously) higher performance. Instead, we want to get the mean, standard deviation, and other properties only including the training set. Let $t'$ denote the training set, which is exclusive of the test set; this is:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{T'} \\displaystyle\\sum_{t'} x_{t'}\n",
    "$$\n",
    "\n",
    "and so on.\n",
    "\n",
    "How does this map back into code? Typically, you'll be doing steps that look like this:\n",
    "\n",
    "```python\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "... training ...\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "```\n",
    "\n",
    "Not all pre-processing functions have this problem—it's only *global* ones. But they are most pre-processing functions, so you do need to take care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minmax scaling and its variants\n",
    "\n",
    "Another popular scaling approach is to use min-max scaling, usually defined as:\n",
    "\n",
    "$$\n",
    "x \\longrightarrow \\frac{x - \\text{max}(x)}{\\text{max}(x) - \\text{min}(x)} \\in (0, 1)\n",
    "$$\n",
    "\n",
    "Once again, this is a *global* scaling, so you want to be careful to only define max and min using training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_mm = preprocessing.MinMaxScaler()\n",
    "X_minmax_scaled = scaler_mm.fit_transform(X)\n",
    "print(f\"The min is {X_minmax_scaled.min():.3f}\")\n",
    "print(f\"The max is {X_minmax_scaled.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variant on minmax scaling is the [`MaxAbsScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler), which scales each feature by its maximum *absolute* value. This means it works with negative value too: values are mapped across several ranges depending on whether negative OR positive values are present. If only positive values are present, the range is [0, 1]. If only negative values are present, the range is [-1, 0]. If both negative and positive values are present, the range is [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both the `MinMaxScaler()` and `MaxAbsScaler()` are prone to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Scaler\n",
    "\n",
    "So what about the situation where you have some outliers? Of course, if they're truly erroneous you may wish to remove them  before you start working with the data (just remember that the test data may have some too!). The other option is to use `RobustScaler()`. What does \"robust\" mean here? Basically that the addition or removal of outliers will not significantly change the character of the transformation. In particular, the centering and scaling statistics of `RobustScaler()` are based on percentiles and are therefore not influenced by a small number of very large marginal outliers. Consequently, the resulting range of the transformed feature values is larger than for the previous scalers. Note that the outliers are retained.\n",
    "\n",
    "Let's see an example straight out of the excellent **scikit-learn** [documentation](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#plot-all-scaling-minmax-scaler-section) with occupancy and income data for housing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "\n",
    "dataset = fetch_california_housing()\n",
    "X_full, y_full = dataset.data, dataset.target\n",
    "feature_names = dataset.feature_names\n",
    "\n",
    "feature_mapping = {\n",
    "    \"MedInc\": \"Median income in block\",\n",
    "    \"HouseAge\": \"Median house age in block\",\n",
    "    \"AveRooms\": \"Average number of rooms\",\n",
    "    \"AveBedrms\": \"Average number of bedrooms\",\n",
    "    \"Population\": \"Block population\",\n",
    "    \"AveOccup\": \"Average house occupancy\",\n",
    "    \"Latitude\": \"House block latitude\",\n",
    "    \"Longitude\": \"House block longitude\",\n",
    "}\n",
    "\n",
    "# Take only 2 features to make visualization easier\n",
    "# Feature MedInc has a long tail distribution.\n",
    "# Feature AveOccup has a few but very large outliers.\n",
    "features = [\"MedInc\", \"AveOccup\"]\n",
    "features_idx = [feature_names.index(feature) for feature in features]\n",
    "X = X_full[:, features_idx]\n",
    "distributions = [\n",
    "    (\"Unscaled data\", X),\n",
    "    (\"Data after standard scaling\", StandardScaler().fit_transform(X)),\n",
    "    (\"Data after min-max scaling\", MinMaxScaler().fit_transform(X)),\n",
    "    (\"Data after max-abs scaling\", MaxAbsScaler().fit_transform(X)),\n",
    "    (\n",
    "        \"Data after robust scaling\",\n",
    "        RobustScaler(quantile_range=(25, 75)).fit_transform(X),\n",
    "    ),\n",
    "    (\n",
    "        \"Data after power transformation (Yeo-Johnson)\",\n",
    "        PowerTransformer(method=\"yeo-johnson\").fit_transform(X),\n",
    "    ),\n",
    "    (\n",
    "        \"Data after power transformation (Box-Cox)\",\n",
    "        PowerTransformer(method=\"box-cox\").fit_transform(X),\n",
    "    ),\n",
    "    (\n",
    "        \"Data after quantile transformation (uniform pdf)\",\n",
    "        QuantileTransformer(\n",
    "            output_distribution=\"uniform\", random_state=random_state\n",
    "        ).fit_transform(X),\n",
    "    ),\n",
    "    (\n",
    "        \"Data after quantile transformation (gaussian pdf)\",\n",
    "        QuantileTransformer(\n",
    "            output_distribution=\"normal\", random_state=random_state\n",
    "        ).fit_transform(X),\n",
    "    ),\n",
    "    (\"Data after sample-wise L2 normalizing\", Normalizer().fit_transform(X)),\n",
    "]\n",
    "\n",
    "# scale the output between 0 and 1 for the colorbar\n",
    "y = minmax_scale(y_full)\n",
    "\n",
    "# plasma does not exist in matplotlib < 1.5\n",
    "cmap = getattr(cm, \"plasma_r\", cm.hot_r)\n",
    "\n",
    "\n",
    "def create_axes(title, figsize=(12, 6)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # define the axis for the first plot\n",
    "    left, width = 0.1, 0.22\n",
    "    bottom, height = 0.1, 0.7\n",
    "    bottom_h = height + 0.15\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter = plt.axes(rect_scatter)\n",
    "    ax_histx = plt.axes(rect_histx)\n",
    "    ax_histy = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the zoomed-in plot\n",
    "    left = width + left + 0.2\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter_zoom = plt.axes(rect_scatter)\n",
    "    ax_histx_zoom = plt.axes(rect_histx)\n",
    "    ax_histy_zoom = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the colorbar\n",
    "    left, width = width + left + 0.13, 0.01\n",
    "\n",
    "    rect_colorbar = [left, bottom, width, height]\n",
    "    ax_colorbar = plt.axes(rect_colorbar)\n",
    "\n",
    "    return (\n",
    "        (ax_scatter, ax_histy, ax_histx),\n",
    "        (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),\n",
    "        ax_colorbar,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_distribution(axes, X, y, hist_nbins=50, title=\"\", x0_label=\"\", x1_label=\"\"):\n",
    "    ax, hist_X1, hist_X0 = axes\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x0_label)\n",
    "    ax.set_ylabel(x1_label)\n",
    "\n",
    "    # The scatter plot\n",
    "    colors = cmap(y)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker=\"o\", s=5, lw=0, c=colors)\n",
    "\n",
    "    # Removing the top and the right spine for aesthetics\n",
    "    # make nice axis layout\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines[\"left\"].set_position((\"outward\", 10))\n",
    "    ax.spines[\"bottom\"].set_position((\"outward\", 10))\n",
    "\n",
    "    # Histogram for axis X1 (feature 5)\n",
    "    hist_X1.set_ylim(ax.get_ylim())\n",
    "    hist_X1.hist(\n",
    "        X[:, 1], bins=hist_nbins, orientation=\"horizontal\", color=\"grey\", ec=\"grey\"\n",
    "    )\n",
    "    hist_X1.axis(\"off\")\n",
    "\n",
    "    # Histogram for axis X0 (feature 0)\n",
    "    hist_X0.set_xlim(ax.get_xlim())\n",
    "    hist_X0.hist(\n",
    "        X[:, 0], bins=hist_nbins, orientation=\"vertical\", color=\"grey\", ec=\"grey\"\n",
    "    )\n",
    "    hist_X0.axis(\"off\")\n",
    "\n",
    "\n",
    "def make_plot(item_idx):\n",
    "    title, X = distributions[item_idx]\n",
    "    ax_zoom_out, ax_zoom_in, ax_colorbar = create_axes(title)\n",
    "    axarr = (ax_zoom_out, ax_zoom_in)\n",
    "    plot_distribution(\n",
    "        axarr[0],\n",
    "        X,\n",
    "        y,\n",
    "        hist_nbins=200,\n",
    "        x0_label=feature_mapping[features[0]],\n",
    "        x1_label=feature_mapping[features[1]],\n",
    "        title=\"Full data\",\n",
    "    )\n",
    "\n",
    "    # zoom-in\n",
    "    zoom_in_percentile_range = (0, 99)\n",
    "    cutoffs_X0 = np.percentile(X[:, 0], zoom_in_percentile_range)\n",
    "    cutoffs_X1 = np.percentile(X[:, 1], zoom_in_percentile_range)\n",
    "\n",
    "    non_outliers_mask = np.all(X > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) & np.all(\n",
    "        X < [cutoffs_X0[1], cutoffs_X1[1]], axis=1\n",
    "    )\n",
    "    plot_distribution(\n",
    "        axarr[1],\n",
    "        X[non_outliers_mask],\n",
    "        y[non_outliers_mask],\n",
    "        hist_nbins=50,\n",
    "        x0_label=feature_mapping[features[0]],\n",
    "        x1_label=feature_mapping[features[1]],\n",
    "        title=\"Zoom-in\",\n",
    "    )\n",
    "\n",
    "    norm = mpl.colors.Normalize(y_full.min(), y_full.max())\n",
    "    mpl.colorbar.ColorbarBase(\n",
    "        ax_colorbar,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        orientation=\"vertical\",\n",
    "        label=\"Colour mapping for values of y\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "make_plot(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Transformations\n",
    "\n",
    "`PowerTransformer()` applies a power transformation to each feature to make the data more Gaussian-like in order to stabilise variance and minimise skewness. Currently the Yeo-Johnson and Box-Cox transforms are supported in **scikit-learn** and the optimal scaling factor is determined via maximum likelihood estimation in both methods (see the {ref}`time-series` chapter for how to estimate $\\lambda$ yourself). By default, `PowerTransformer()` applies zero-mean, unit variance normalisation. Note that Box-Cox can only be applied to strictly positive data.\n",
    "\n",
    "#### The Box-Cox Transform\n",
    "\n",
    "This transform actually nests power transforms and logarithms, depending on the value of a parameter that is usually denoted $\\lambda$. The transform is given by\n",
    "\n",
    "$$\n",
    "y_t'  =\n",
    "    \\begin{cases}\n",
    "      \\ln(y_t) & \\text{if $\\lambda=0$};  \\\\\n",
    "      (y_t^\\lambda-1)/\\lambda & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "#### The Yeo-Johnson Transform\n",
    "\n",
    "$$\n",
    "y_i = \n",
    "\\begin{cases}\n",
    "    ((y_i + 1)^{\\lambda} - 1) / \\lambda & \\text{if } \\lambda \\neq 0 \\text{ and } y_i \\geq 0 \\\\\n",
    "    -((-y_i + 1)^{2 - \\lambda} - 1) / (2 - \\lambda) & \\text{if } \\lambda \\neq 2 \\text{ and } y_i < 0 \\\\\n",
    "    \\ln(y_i + 1) & \\text{if } \\lambda = 0 \\\\\n",
    "    -\\ln(-y_i + 1) & \\text{if } \\lambda = 2\n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing transforms\n",
    "\n",
    "It can be quite confusing to keep track of all of these transforms! To give you a better sense of when to use which transform, take a look at the figure below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "\n",
    "N_SAMPLES = 1000\n",
    "FONT_SIZE = 10\n",
    "BINS = 30\n",
    "\n",
    "bc = PowerTransformer(method=\"box-cox\")\n",
    "yj = PowerTransformer(method=\"yeo-johnson\")\n",
    "# n_quantiles is set to the training set size rather than the default value\n",
    "# to avoid a warning being raised by this example\n",
    "qt = QuantileTransformer(\n",
    "    n_quantiles=500, output_distribution=\"normal\", random_state=random_state\n",
    ")\n",
    "size = (N_SAMPLES, 1)\n",
    "\n",
    "\n",
    "# lognormal distribution\n",
    "X_lognormal = prng.lognormal(size=size)\n",
    "\n",
    "# chi-squared distribution\n",
    "df = 3\n",
    "X_chisq = prng.chisquare(df=df, size=size)\n",
    "\n",
    "# weibull distribution\n",
    "a = 50\n",
    "X_weibull = prng.weibull(a=a, size=size)\n",
    "\n",
    "# gaussian distribution\n",
    "loc = 100\n",
    "X_gaussian = prng.normal(loc=loc, size=size)\n",
    "\n",
    "# uniform distribution\n",
    "X_uniform = prng.uniform(low=0, high=1, size=size)\n",
    "\n",
    "# bimodal distribution\n",
    "loc_a, loc_b = 100, 105\n",
    "X_a, X_b = prng.normal(loc=loc_a, size=size), prng.normal(loc=loc_b, size=size)\n",
    "X_bimodal = np.concatenate([X_a, X_b], axis=0)\n",
    "\n",
    "\n",
    "# create plots\n",
    "distributions = [\n",
    "    (\"Lognormal\", X_lognormal),\n",
    "    (\"Chi-squared\", X_chisq),\n",
    "    (\"Weibull\", X_weibull),\n",
    "    (\"Gaussian\", X_gaussian),\n",
    "    (\"Uniform\", X_uniform),\n",
    "    (\"Bimodal\", X_bimodal),\n",
    "]\n",
    "\n",
    "colors = [\"#D81B60\", \"#0188FF\", \"#FFC107\", \"#B7A2FF\", \"#000000\", \"#2EC5AC\"]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=8, ncols=3, figsize=(8, 15))\n",
    "axes = axes.flatten()\n",
    "axes_idxs = [\n",
    "    (0, 3, 6, 9),\n",
    "    (1, 4, 7, 10),\n",
    "    (2, 5, 8, 11),\n",
    "    (12, 15, 18, 21),\n",
    "    (13, 16, 19, 22),\n",
    "    (14, 17, 20, 23),\n",
    "]\n",
    "axes_list = [(axes[i], axes[j], axes[k], axes[m]) for (i, j, k, m) in axes_idxs]\n",
    "\n",
    "\n",
    "for distribution, color, axes in zip(distributions, colors, axes_list):\n",
    "    name, X = distribution\n",
    "    X_train, X_test = train_test_split(X, test_size=0.5)\n",
    "\n",
    "    # perform power transforms and quantile transform\n",
    "    X_trans_bc = bc.fit(X_train).transform(X_test)\n",
    "    lmbda_bc = round(bc.lambdas_[0], 2)\n",
    "    X_trans_yj = yj.fit(X_train).transform(X_test)\n",
    "    lmbda_yj = round(yj.lambdas_[0], 2)\n",
    "    X_trans_qt = qt.fit(X_train).transform(X_test)\n",
    "\n",
    "    ax_original, ax_bc, ax_yj, ax_qt = axes\n",
    "\n",
    "    ax_original.hist(X_train, color=color, bins=BINS)\n",
    "    ax_original.set_title(name, fontsize=FONT_SIZE)\n",
    "    ax_original.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n",
    "\n",
    "    for ax, X_trans, meth_name, lmbda in zip(\n",
    "        (ax_bc, ax_yj, ax_qt),\n",
    "        (X_trans_bc, X_trans_yj, X_trans_qt),\n",
    "        (\"Box-Cox\", \"Yeo-Johnson\", \"Quantile transform\"),\n",
    "        (lmbda_bc, lmbda_yj, None),\n",
    "    ):\n",
    "        ax.hist(X_trans, color=color, bins=BINS)\n",
    "        title = \"After {}\".format(meth_name)\n",
    "        if lmbda is not None:\n",
    "            title += \"\\n$\\\\lambda$ = {}\".format(lmbda)\n",
    "        ax.set_title(title, fontsize=FONT_SIZE)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n",
    "        ax.set_xlim([-3.5, 3.5])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Transform\n",
    "\n",
    "`QuantileTransformer()` applies a non-linear transformation such that the probability density function of each feature will be mapped to a uniform or Gaussian distribution. In this case, all the data, including outliers, will be mapped to a uniform distribution with the range [0, 1], making outliers indistinguishable from inliers. (Remember, `RobustScaler()` and `QuantileTransformer()` are robust to outliers in the sense that adding or removing outliers in the training set will yield approximately the same transformation.) A quantile transform smooths out unusual distributions and is less influenced by outliers than scaling methods—it does, however, distort correlations and distances within and across features.\n",
    "\n",
    "Quantile transformations work by first creating an estimate of the cumulative distribution function of a feature and then using it to map the original values to a uniform or Gaussian distribution. Features values of new or unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution.\n",
    "\n",
    "You can see examples of what happens to an input distribution when quantised in the figure above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing missing values\n",
    "\n",
    "Rarely do real world data have no issues, and missing values are among the most common of those. You hope that if you do have missing data, that it is missing completely at random, then you have some guarantees that you can still do succesful inference (see {cite:ts}`hastie2009elements` for a full discussion). If you do have data missing completely at random then you have three options.\n",
    "\n",
    "The first and most basic is to discard the missing values (delete those rows entirely), but you may be throwing away other, useful data from other columns. Another option is to use an algorithm that is comfortable with missing values (some examples include HDBSCAN for clustering, and the Decision Tree Regressor and Classifier). The third is to impute missing values.\n",
    "\n",
    "Three popular approaches for imputing missing values are:\n",
    "\n",
    "- Average Imputation: Replace missing values with the mean or median value of the feature. This is simple and effective.\n",
    "- Regression Imputation: Predict missing values using regression models based on other correlated features. More accurate but more computationally costly.\n",
    "- K-Nearest Neighbor Imputation: Predict the missing values by averaging or interpolating using the values of the missing value’s nearest neighbors.\n",
    "\n",
    "Note that the first and last use the feature only; the second relies on other features too.\n",
    "\n",
    "The most basic approach, the first, is covered by the `SimpleImputer()` class in **scikit-learn**. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings. Here's an example of using the `SimpleImputer()` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "imp.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's an example of the *last* approach, using K nearest neighbours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "nan = np.nan\n",
    "X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more imputation options than **scikit-learn**, check out [**feature-engine**](https://feature-engine.trainindata.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Pipeline are a tool that help you efficiently chain multiple transformations and (in the final step) an estimator into one piece of code logics. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalisation, and classification. Pipeline serves multiple purposes here:\n",
    "\n",
    "- Convenience and encapsulation: You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "\n",
    "- Joint parameter selection: You can grid search over parameters of all estimators in the pipeline at once.\n",
    "\n",
    "- Safety: Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n",
    "\n",
    "All of the elements in a pipeline, except the last one, must be transformers (i.e. must have a transform method).\n",
    "\n",
    "A pipeline can be built using a list of (key, value) pairs, where the key is a string containing the name you want to give a step step and value is a transformation or an estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe = Pipeline([(\"standardise\", StandardScaler()), (\"svr\", SVC(kernel=\"rbf\"))])\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it to some data that are not centered. We'll use the wine data classification problem. (More info on the wine dataset may be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The mean is {X_train.mean():.3f} and the standard deviation {X_train.std():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the scaler and SVR fitted, we can now easily use the fitted pipeline to score on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The accuracy of the pipeline classification is {pipe.score(X_test, y_test):.1%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is different to what you'd get if you just trained an SVC and scored it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_svr = SVC(kernel=\"rbf\").fit(X_train, y_train)\n",
    "print(\n",
    "    f\"The accuracy of the un-standardised classification is {fitted_svr.score(X_test, y_test):.1%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you just want to use default names for the steps in your pipeline, you may use the `make_pipeline` function instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe_default_names = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\"))\n",
    "pipe_default_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering or feature extraction or feature discovery is the process of extracting features (usually, columns of data) from raw data to support training a downstream statistical model. To give a really concrete example: imagine you are tasked with building a model that predicts when flight tickets get sold, but your raw data only include the date and time of the flight and the current date: you can imagine that the very first thing you'd do would be to combine these two to create a new feature that is time until the flight actually departs. This is the essence of feature engineering.\n",
    "\n",
    "Note that, with deep learning, you may not need as much feature engineering as the algorithm will effectively work out how best to combine raw features under the hood.\n",
    "\n",
    "There are a few popular methods for creating features. Of course you can create features yourself, intelligently, using what you know. Our flights example does just this. But there are more programmatic ways.\n",
    "\n",
    "One progrommatic way to create features to throw a couple of features together with a bunch of mathematical functions and see what comes out of the other end. Another way that people do this is using a genetic algorithm, for which you should check out the [**tpot**](https://github.com/EpistasisLab/tpot) package. Let's see an example of the former approach using [**feature-engine**](https://feature-engine.trainindata.com/). First, some fake data with example features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from feature_engine.creation import MathFeatures\n",
    "\n",
    "df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"Name\": [\"tom\", \"nick\", \"krish\", \"jack\"],\n",
    "        \"City\": [\"London\", \"Manchester\", \"Liverpool\", \"Bristol\"],\n",
    "        \"Age\": [20, 21, 19, 18],\n",
    "        \"Marks\": [0.9, 0.8, 0.7, 0.6],\n",
    "        \"dob\": pd.date_range(\"2020-02-24\", periods=4, freq=\"T\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the `MathFeatures` class to generate a transformer, and then `fit_transform()` to generate new downstream features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = MathFeatures(\n",
    "    variables=[\"Age\", \"Marks\"],\n",
    "    func=[\"sum\", \"prod\", \"min\", \"max\", \"std\"],\n",
    ")\n",
    "\n",
    "df_t = transformer.fit_transform(df)\n",
    "df_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series features tend to need different types of engineering. Let's see an example. First, some made up features in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_series = {\n",
    "    \"ambient_temp\": [31.31, 31.51, 32.15, 32.39, 32.62, 32.5, 32.52, 32.68],\n",
    "    \"irradiation\": [0.51, 0.79, 0.65, 0.76, 0.42, 0.49, 0.57, 0.56],\n",
    "}\n",
    "\n",
    "ts_series = pd.DataFrame(ts_series)\n",
    "ts_series.index = pd.date_range(\"2020-05-15 12:00:00\", periods=8, freq=\"15min\")\n",
    "ts_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some extra features using [**feature-engineer**](https://feature-engine.trainindata.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.timeseries.forecasting import WindowFeatures\n",
    "\n",
    "win_f = WindowFeatures(\n",
    "    window=[\"30min\", \"60min\"],\n",
    "    functions=[\"mean\", \"max\", \"std\"],\n",
    "    freq=\"15min\",\n",
    ")\n",
    "\n",
    "ts_series_tr = win_f.fit_transform(ts_series)\n",
    "ts_series_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we now have a whole load more features constructed from our existing set. And, even better, we have some control over what kinds of aggregations are created, and over what period.\n",
    "\n",
    "This has just been the tiniest possible introduction to feature engineering. For more, see the [**feature-engine**](https://feature-engine.trainindata.com/) documentation or the excellent (free) book, the [Python Feature Engineering Cookbook](https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook) {cite:ps}`galli2022python`, or the package [**featuretools**](https://featuretools.alteryx.com/en/stable/). For time series, you might want to check out [**tsfresh**](https://tsfresh.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical features\n",
    "\n",
    "In economics and traditional statistics, we normally use the language of \"fixed effects\" to describe how to enable OLS to deal with categorical variables. In machine learning, the convention is a little different and we talk of \"encoding categorical variables\", which is to say turning categories such as `[\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]` into a matrix of integers.\n",
    "\n",
    "(Note that you can read much more about using categories with **pandas** in Chapter {ref}`categorical-data`.)\n",
    "\n",
    "### Existing categories into integer features\n",
    "\n",
    "There are a couple of popular ways to turn categories into a matrix of integers. One is to use **pandas** `pd.get_dummies()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.Series(list(\"abc\"))\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(categories, dtype=float)  # use dtype bool for true and false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pandas** also has the `.cat.codes` accessor for data of type `category`.\n",
    "\n",
    "Another approach is to use the functionality of **scikit-learn** directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "array_cats = [\n",
    "    [\"male\", \"from US\", \"uses Safari\"],\n",
    "    [\"female\", \"from Europe\", \"uses Firefox\"],\n",
    "    [\"male\", \"from Europe\", \"uses Opera\"],\n",
    "    [\"male\", \"from Asia\", np.nan],\n",
    "]\n",
    "enc.fit_transform(array_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `OrdinalEncoder()` turns each categorical feature into an integer in the range 0 to n_categories - 1. Note that the NaN value has been passed through: you can pass, for example, `encoded_missing_value=-1` as a keyword argument to ordinal encoded should you wish to encode NaNs too.\n",
    "\n",
    "Another two important keyword arguments are `min_frequency` and `max_categories`, which, separately, can determine how many categories are created in the obvious ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretisation of continuous features\n",
    "\n",
    "We already saw one way to discretise continuous features using `pd.cut()` and `pd.qcut()`: you can read more about these in the Chapters on {ref}`categorical-data` and {ref}`data-transformation`.\n",
    "\n",
    "**scikit-learn** also offers some ways to do this, for example k-bins discretiser. It is used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_three_features = np.array([[-3.0, 5.0, 15], [0.0, 6.0, 14], [6.0, 3.0, 11]])\n",
    "(\n",
    "    preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode=\"ordinal\").fit_transform(\n",
    "        data_three_features\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each feature, the bin edges are computed during fit and, together with the number of bins, they will define the intervals. Use the `strategy=` keyword argument to choose \"uniform\", \"quantile\", or \"kmeans\" as the binning strategy. You can see more in the **scikit-learn** [documentation](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-discretization)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "codeforecon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
