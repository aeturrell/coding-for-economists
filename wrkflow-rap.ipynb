{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(wrkflow-rap)=\n",
    "# Reproducible Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This chapter will tell you about reproducible analysis and what are sometimes called \"reproducible analytical pipelines\", also known as RAPs. This chapter has benefitted from the \"[Quality Assurance of Code for Analysis and Research](https://best-practice-and-impact.github.io/qa-of-code-guidance)\" book produced by the Best Practice and Impact division of the Office for National Statistics and \"The Turing Way\", a [guide to reproducibility](https://the-turing-way.netlify.app) produced by the Alan Turing Institute.\n",
    "\n",
    "Reproducibility can mean a lot of things to a lot of different people in a lot of different contexts. Here, we're going to think about the reproducibility of analysis performed by code. That excludes external validity and although it might include robust coding practices, it doesn't include robustness checks.\n",
    "\n",
    "It does include Reproducible Analytical Pipelines (RAPs), which are automated analytical processes. At their best, RAPs use elements of software engineering best practice to ensure that analysis is reproducible.\n",
    "\n",
    "There are different levels and aspects of reproducibile analysis with code, which we will step through in the rest of this chapter, but they form a hierarchy:\n",
    "\n",
    "- modular code\n",
    "- sensible, isolated project structure\n",
    "- an unmodified version of the raw data is retained; outputs can be regenerated\n",
    "- operations should form a reproducible analytical pipeline\n",
    "- the reproducible analytical pipeline can be automatically executed\n",
    "- code language is reproducible (eg `Python=3.8.12` in pyproject.toml file)\n",
    "- code language packages are reproducible (eg `pandas=1.3.3` in pyproject.toml file)\n",
    "- code language packages' dependencies are reproducible (eg pandas depends on `numpy=1.21` as specified in poetry.lock file)\n",
    "- operating system is reproducible (as specified in a Docker file)\n",
    "\n",
    "A lot of the value to be had in reproducible code can be achieved by just the first five. Nailing all but the last one is a high standard and probably the right spot in the trade-off for most projects. Doing all of these is the rarely achieved gold standard for reproducibility (but could be overkill in many cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Modular Code\n",
    "\n",
    "(You may have already met this idea before as the DRY, or \"don't repeat yourself\", concept).\n",
    "\n",
    "How you structure your code can itself aid reproducibility. Let's imagine you have a situation where you want to perform two similar analytical tasks in which the only thing that changes are the inputs and outputs. Let's say, for example, that you want to perform a particular set of operations, perhaps the following:\n",
    "\n",
    "> find the mean from the second entry in a list onwards, add 1, divide by the number of entries in the reduced length list and then print the answer as a score\n",
    "\n",
    "Here's one example of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "list1 = [3, 4, 5, 6, 7, 8]\n",
    "score1 = (np.mean(list1[1:]) + 1) / 5\n",
    "print(f\"The score is {score1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now we have imagine we have a second set of variables we'd like to run the same logic on. Naively, you might copy and paste the same code again but replace the list with the new one, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = [16, 12, 88, 1, 9, 7, 32, 5]\n",
    "score2 = (np.mean(list2[1:]) + 1) / 5\n",
    "print(f\"The score is {score2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this is that we repeated the code even though we wanted to perform the same operation, which is not very efficient.\n",
    "\n",
    "We also expect `score1` and `score2` to be using the same operations, but we can't be sure now because *the code is different*. And indeed, in this case, the copy and paste philosophy has gone wrong. Did you spot the mistake? Instead of dividing by the number of entries in the reduced length list, our copy and paste job saw us dividing by the number of entries in the first case (creating `score1`).\n",
    "\n",
    "What we should have done is create modular code, where the same logic is not repeated but *re-used*. In this case, that's best done with a function like follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(list_input):\n",
    "    \"\"\"find the mean from the second entry in a list onwards,\n",
    "    add 1, divide by the number of entries in the reduced\n",
    "    length list and then print the answer as a score.\n",
    "\n",
    "    Args:\n",
    "        list_input (list[float]): A list of numbers\n",
    "    Returns:\n",
    "        score (float): A computed score\n",
    "    \"\"\"\n",
    "    reduced_list = list_input[1:]\n",
    "    score = (np.mean(reduced_list) + 1) / len(reduced_list)\n",
    "    print(f\"The score is {score:.2f}\")\n",
    "\n",
    "\n",
    "# Just to show it works:\n",
    "compute_score(list1)\n",
    "compute_score(list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now be sure that we get *exactly* the same set of operations applied each time because we put the logic into one function that is called for both lists. And, to avoid any copy and paste errors, we switched from hard-coding the divisor to making it part of the logic of the function (by dividing by `len(reduced_list)`).\n",
    "\n",
    "By putting our logic in a function, we've also made what we are trying to do clearer to the reader‚Äîand remember, coders, that reader is most likely to be you in a few months time.\n",
    "\n",
    "Functions help with reproducibile analytical pipelines because you can be sure that even if your inputs change slightly, the same logic or set of operations will be run.\n",
    "\n",
    "Of course, functions do not have to be in the same script (the same `.py` file) as the one you are currently writing your code in. Python allows you to import functions from other scripts.\n",
    "\n",
    "A classic example would be if you had a project that was structured like this:\n",
    "\n",
    "```bash\n",
    "üìÅ project\n",
    "| main.py\n",
    "| utilities.py\n",
    "| figures.py\n",
    "| README.md\n",
    "```\n",
    "\n",
    "where 'project' is a folder, and the files within it are three Python scripts and a markdown document. Your utilities script might have a really useful function in it that you'd like to use in main.py and figures.py too. You can do this! It's possible to do from 'within' your other scripts. It's also surprisingly simple. Your utilities script might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents of utilities.py file\n",
    "def really_useful_func(number):\n",
    "    \"\"\"Performs a really useful operation on input number\n",
    "    and returns the result.\"\"\"\n",
    "    return number * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While, to use this function in your main.py script, you'd have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents of main.py\n",
    "import utilities as util_mod\n",
    "\n",
    "print(util_mod.really_useful_func(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, just as valid, you could import the same function from utilities in a different way in figures.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import really_useful_func\n",
    "\n",
    "print(really_useful_func(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technical jargon for what you've just done is to use `utilities.py` as a *module* rather than as a script.\n",
    "\n",
    "```{note}\n",
    "If your script is in a different directory, you will need a relative import. Let's say you've set your project's default directory to the folder \"project\", and the module you'd like to import is in a directory useful, so that it's path is `project/useful/utilities.py`. As before, you're running code from the default directory, `project/main.py`. Then you'll need to import the script within `main.py` using the following: `from useful import utilities as utils`.\n",
    "```\n",
    "\n",
    "The reasons for wanting to be able to import functions (or other code) from other scripts are exactly the same as for functions within scripts: so that you only need write the function once to use it everywhere, and so that you can be sure you are running exactly the same set of operations each time.\n",
    "\n",
    "```{note}\n",
    "If you are in the middle of a coding session and you change the code in a module (external script), your Python kernel will not necessarily know that you have changed the external script. You can either restart the Python kernel, but you lose your variables, or use the built-in `importlib` Python package. For example, to reload an external module named `utilities.py` that had been imported via `import utilities as utils`, you would run `import importlib` and, when you wanted to refresh the module, `importlib.reload(utils)`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "\n",
    "The first principle to guide you should be that **each project lives in its own folder**. Everything for that project will appear somewhere under the top level directory. Do not mix projects in the same folder directory.\n",
    "\n",
    "A typical project structure might look like:\n",
    "\n",
    "```bash\n",
    "üìÅ project\n",
    "|-- README.md\n",
    "|-- requirements.txt\n",
    "|-- data/\n",
    "|   -- raw_data.csv\n",
    "|-- src/\n",
    "|   -- utilities.py\n",
    "|   -- data_cleaning.py\n",
    "|   -- analysis.py\n",
    "|   -- generate_plots_and_tables.py\n",
    "|-- results/\n",
    "|   -- cleaned_data.csv\n",
    "|   -- derived_data.csv\n",
    "|-- outputs/\n",
    "|   -- latex/\n",
    "|       -- paper.tex\n",
    "|       -- slides.tex\n",
    "|   -- floats/\n",
    "|       -- table.tex\n",
    "|       -- time_series_plot.pdf\n",
    "```\n",
    "\n",
    "Let's take a quick tour through this example project's structure, all of which sits in the \"project\" directory.\n",
    "\n",
    "First we have the README.md, a file full of instructions in a simple text format called *markdown* (=\".md\"). Markdown is kind of like a code language for text and is commonly used to write information in coding projects. It can be \"rendered\" (that is, can be displayed not as code but in a form that is intended to be read) by Visual Studio Code using the markdown all in one extension (see the {ref}`code-where` chapter for more on markdown).\n",
    "\n",
    "The next file, \"requirements.txt\", is one we'll be seeing much more of later in this chapter. For now, all you need to do know is that it is a list of all of the Python packages needed to run the project. Sometimes it might be called \"environment.yml\" instead, or sometimes \"pyproject.toml\".\n",
    "\n",
    "Next we have the data, which is the data as you received it, no matter how poor the format (wishful thinking in this example that it might be a clean csv file and not 400 Excel files!). A really important principle here is that **you should not alter raw data** as it comes to you, but instead treat it as read-only. This is so that you can always trace a path from the raw data you began with to the finished product. Incoming data can and does change surprisingly often so if someone else were to do the \"same\" analysis as you and find a different answer, you want to be able to rule out starting with different input datasets as the cause. Or you may find that your data cleaning script needs adjusting, and you need to repeat all of the operations in your project. Or other good reasons. Always keep a copy of the raw data.\n",
    "\n",
    "The next folder is \"src\", which is short for \"source code\", and it's where your code scripts (.py files) or possibly Jupyter Notebooks (.ipynb files) live. These are nicely separate into the functions they perform: cleaning the data, performing the analysis, and summarising the analysis into figures and tables (also known as floats). The other script, \"utilities.py\", might be imported as module that all of the other scripts can use.\n",
    "\n",
    "The results from running the first two scripts are in the \"results\" folder. First we have a file that saves the results from the intermediate step of the cleaning the data (courtesy of running \"data_cleaning.py\"), then we have the results from \"analysis.py\", the derived data. Derived data might be, for example, the final tabular dataset that you use for a regression.\n",
    "\n",
    "Next up is outputs, which is split into \"latex\" and \"floats\".\n",
    "\n",
    "Latex is a typesetting language that is used for writing reports, slides, journal articles, and more. Importantly for coding and replicability, it can be automated (in a way that, say, Word can't be). Another option here would be markdown, which can also be exported to a wide variety of output formats. The text in a Latex report has to be written, we can't automate that (yet), but the figures and tables that are generated by the \"generate_plots_and_tables.py\" script can be automatically inserted. The same goes for slides written in a variant of Latex called Beamer. You can find more information on the process of automating reports and outputs in the chapter on {ref}`wrkflow-automating-outputs`.\n",
    "\n",
    "Floats means, here at least, tables and figures. These also live in the outputs directory, and are put there by the \"generate_plots_and_tables.py\" script.\n",
    "\n",
    "So that's the structure. There is another important principle here, which follows on from the first: **you should be able to dispose of the results, figures, and tables (ie delete them), without worrying**. If you are working on a project and deleting any outputs that are generated by code scripts would scupper everything, then your project is not reproducible. (This is another reason why you should always keep an \"untouched\" version of the raw data.) If it's feasible to do so time-wise, it is good practice to delete and regenerate your outputs regularly to test that your project is indeed reproducible.\n",
    "\n",
    "There are, of course, many variations on this example project structure. You can find \"cookiecutter\" templates out there that will help you structure your own project (some involve answering some questions about what you need, and what you'd like to call project, and so on), or just get inspiration. Some good templates to check out that can help you structure your project are the UK Government data science, [govcookiecutter](https://github.com/ukgovdatascience/govcookiecutter) and this comprehensive Python data science project template known as [cookiecutter-data-science](http://drivendata.github.io/cookiecutter-data-science/). For advanced users looking to build Python packages that others can install, the [hypermodern Python cookiecutter](https://cookiecutter-hypermodern-python.readthedocs.io/en/2021.7.15/) is packed full of amazing features and comes highly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducible Analytical Pipelines\n",
    "\n",
    "We've seen how functions and modules can aid the reproducibility of logic. We've seen the kind of folder and file structure that can aid reproducibility of projects too. But the *order* of operations matters as well! Pipelines organise your analysis in a series of steps that, when executed, perform the same series of operations in the same order. If you arrange your code in a reproducible analytical pipelines, you will know that you can send your code to someone else and, when they run it, the same steps will be executed in the same order.\n",
    "\n",
    "In this section, we'll look at the concept in general as well as some tools that can aid you in creating reproducible analytical pipelines.\n",
    "\n",
    "The first principle of RAPs may be a surprise: you shouldn't use the interactive console interactively (eg by jamming, re-running lines, etc, as you go). A RAP should be able to go from start to finish with zero human interaction during execution, and produce consistent results at the end. This is different to when you're still developing a project, and the interactive console is super useful because you're trying things out. But RAP is for prosperity and needs to work without your intervention!\n",
    "\n",
    "### Directed Acyclic Graphs\n",
    "\n",
    "Directed acyclic graphs pop up in lots of contexts. Here, in a RAP context, we're talking about them because the execution order of your code and scripts should form a directed acyclic graph (DAG). The word \"graph\" here can be confusing: it's referring to the formal notion of a graph from the branch of mathematics called graph theory, which is the study of pairwise relations between objects. A graph in this context is made up of vertices (also called nodes or points) that are connected by edges (also called links or lines). A DAG is a directed graph with no directed cycles (closed loops). It consists of vertices and edges with each edge directed from one vertex to another, such that following the directions will never form a closed loop. To summarise: \"directed\" means that edges between nodes have a direction and you can only traverse the graph in that direction (often shown by arrows in diagrams) and \"acyclic\": means there are no cycles so that, for example, a node A can‚Äôt depend on a node B when B depends on A.\n",
    "\n",
    "The point is that the set of operations that are performed when following a DAG proceeds like water flowing through a series of rapids: one step after another, with no looping back.\n",
    "\n",
    "This is all a bit abstract and so it's a lot easier to just take a look at a visualisation of one. We'll use the example from the previous section boiled down to only the code scripts (.py files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide input\n",
    "# Warning: This has a dependency on GraphViz\n",
    "# On mac, using homebrew, brew install graphviz\n",
    "import graphviz\n",
    "import string\n",
    "\n",
    "dot = graphviz.Digraph(comment=\"DAG example\")\n",
    "labels_list = [\"Utilities\", \"Data Cleaning\", \"Analysis\", \"Generate Plots and Tables\"]\n",
    "names_list = string.ascii_uppercase[: len(labels_list)]\n",
    "labels_to_names = dict(zip(labels_list, names_list))\n",
    "for name, label in zip(names_list, labels_list):\n",
    "    dot.node(name, label)\n",
    "\n",
    "list_of_connections = [\n",
    "    (\"Utilities\", \"Data Cleaning\"),\n",
    "    (\"Utilities\", \"Analysis\"),\n",
    "    (\"Utilities\", \"Generate Plots and Tables\"),\n",
    "    (\"Data Cleaning\", \"Analysis\"),\n",
    "    (\"Analysis\", \"Generate Plots and Tables\"),\n",
    "]\n",
    "\n",
    "list_of_connections = [\n",
    "    (labels_to_names[a], labels_to_names[b]) for a, b in list_of_connections\n",
    "]\n",
    "dot.edges(list_of_connections)\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "If you are running this code yourself and want to run the code that creates the diagram above, you should be aware that the diagram above has a dependency on a non-Python library called graphviz that you will need to install separately (in *addition* to the Python library called graphviz, which can be installed using `pip install graphviz`). On Mac, you can use homebrew to run `brew install graphviz` on the command line. `sudo apt install graphviz` works on some flavours of Linux. You can download Windows installers and find more information about this [here](https://www.graphviz.org/download/).\n",
    "```\n",
    "\n",
    "Here we see the execution order. utilities is the antecedent of every step because it is imported by all three of the other scripts. The rest of the DAG is: data cleaning, analysis, and, finally, generate plots and tables. Fancier DAGs might also show the input data, processed data, results, and outputs on the diagram too.\n",
    "\n",
    "Why is turning your project into a DAG important? Because, to be able to reproduce what you did, you must be able to clear define what the steps were that took raw data at the start and output the finished product at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripting Reproducible Analytical Pipelines\n",
    "\n",
    "There's still a manual step in the above logic. Even with the raw data and the folder structure, someone needs to come along and remember to run the (right) three scripts in the right order. But there's no need to leave running the scripts in the right order to chance.\n",
    "\n",
    "There are a few options to automate the execution order of the scripts. One is to write all the scripts as modules with a main script, \"main.py\", that runs everything else. The contents of main.py might look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\" main.py: Master script for project \"project\".\n",
    "\"\"\"\n",
    "\n",
    "import data_cleaning as dc\n",
    "import analysis as analysis\n",
    "import generate_plots_and_tables as gen_outputs\n",
    "\n",
    "dc.run()\n",
    "analysis.run()\n",
    "gen_outputs.run()\n",
    "\n",
    "```\n",
    "\n",
    "With this arrangement, all the user has to know (perhaps from the README.md) is that they need to run \"main.py\" and it will take care of the rest. The code writer (often the same person as the user, but not always) does need to take the extra step of creating the relevant \"run\" functions in each of the three script files too though. And they need to ensure that *they* use main.py when generating their results (rather than running the scripts separately), because otherwise there may be differences.\n",
    "\n",
    "You can see an example project that uses the script method [here](https://github.com/aeturrell/cookie-cutter-research-project)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make for Reproducible Analytical Pipelines\n",
    "\n",
    "Make is a command line tool (to learn more about what the command line is, check out {ref}`wrkflow-command-line`) that helps with automating the execution of a series of processes‚Äîexactly what we need in a RAP. Make does its magic via a command line script. Honestly, it's pretty fussy and complicated, which perhaps explains why it's not more popular, but it's well worth considering as one option for gold-plated reproducible analytical pipelines.\n",
    "\n",
    "It's worth saying up front that Make has little time for pipelines that produce more than one output per input dataset + script combination. This can be puzzling initially, but it's actually probably good practice to have one output per unique combination of inputs. The place where it makes less sense, perhaps, is if you have one script that produces all of your figures and tables.\n",
    "\n",
    "Make is not a Python library, so you'll need to download and install it separately. The installation instructions are different on different operating systems: you can find information on how to install Make [here](https://www.gnu.org/software/make/). On Mac, if you use homebrew, the command to install it is `brew install make`. Similar commands exist for Linux. In Windows, you can use the [chocolatey](https://chocolatey.org/) package manager and then `choco install make` or work in the [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/install).\n",
    "\n",
    "There really is no better tutorial for Make out there than the one that the Alan Turing Institute has put together [here](https://the-turing-way.netlify.app/reproducible-research/make/make-examples.html), so there's little point giving another tutorial here. It does assume some knowledge of command line scripting and version control, but it's pretty comprehensive and a good place to go to see how to use Make for RAP.\n",
    "\n",
    "To give a flavour of how Make works, we'll look at just one step from the Turing example. The most common thing you'd do with a project that uses Make is navigate to the project directory and use the command line to enter `make` and, sometimes, `make clean`. The repository in this example looks like this:\n",
    "\n",
    "```bash\n",
    "üìÅ project\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ input_file_1.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ input_file_2.csv\n",
    "‚îú‚îÄ‚îÄ LICENSE\n",
    "‚îú‚îÄ‚îÄ Makefile\n",
    "‚îú‚îÄ‚îÄ output/\n",
    "‚îú‚îÄ‚îÄ README.md\n",
    "‚îú‚îÄ‚îÄ report/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ report.tex\n",
    "‚îî‚îÄ‚îÄ scripts/\n",
    "    ‚îî‚îÄ‚îÄ generate_histogram.py\n",
    "```\n",
    "\n",
    "The recipe for Make is put into a file called \"Makefile\" within your project folder:\n",
    "\n",
    "```bash\n",
    "# Contents of \"Makefile\" for analysis report\n",
    "\n",
    ".PHONY: all clean\n",
    "\n",
    "all: output/report.pdf\n",
    "\n",
    "output/figure_1.png: data/input_file_1.csv scripts/generate_histogram.py\n",
    "\tpython scripts/generate_histogram.py -i $< -o $@\n",
    "\n",
    "output/figure_2.png: data/input_file_2.csv scripts/generate_histogram.py\n",
    "\tpython scripts/generate_histogram.py -i $< -o $@\n",
    "\n",
    "output/report.pdf: report/report.tex output/figure_1.png output/figure_2.png\n",
    "\tcd report/ && pdflatex report.tex && mv report.pdf ../output/report.pdf\n",
    "\n",
    "clean:\n",
    "\trm -f output/report.pdf\n",
    "\trm -f output/figure_*.png\n",
    "```\n",
    "\n",
    "Let's walk through this and then we'll talk about what it does. (NB: Make uses tabs, not spaces, for continued lines.) Bear in mind, Make is going to execute a RAP for us, so various bits of what's in the Makefile are going to relate to parts of the process.\n",
    "\n",
    "First up, phony tells Make that \"all\" and \"clean\" are not the names of outputs from the project, but commands. \"all\" is defined by `all: output/report.pdf` and is the first target‚Äîwhich means that when `Make` is run without any other information, it will be called. You can see further down the page what \"all\" does: it uses the .tex file and two figures to run `pdflatex` on `report.tex`, thus compiling the latex to a pdf. (It also does some sensible moving things around in the file system.)\n",
    "\n",
    "\"all\" depends on the figures being there. So, if they aren't, it will run the previous two commands listed in the makefile to create them: `output/figure_1.png` and `output/figure_2.png`. These rely on the input csv data and generate_histogram.py script being present and then run the scripts (remember, telling the command line `python script.py` runs the `script.py` from top to bottom). `-i` is a flag that precedes the name of an input, but rather than type out the name of the csv again, a shorthand has been used: `$<`, which just means \"look at the first thing in the command\". `-o` is the flag that precedes the name of an output but again a shorthand is used; `$@` means use the name of the target (here `output/figure_1.png` or `output/figure_2.png`).\n",
    "\n",
    "Finally, we mentioned earlier that it's good practice to delete your ouputs and check that your RAP can re-execute and produce everything again. \"clean\" is the command that does this (via `make clean`). `rm` is command-line speak for remove, and `*` is a wildcard character that can mean anything so here it deletes `figure_1.png` and `figure_2.png` if they exist.\n",
    "\n",
    "One of the nice features of Make is that it can auto-generate the DAG that is defined in the Makefile. Here's the makefile for this example rendered as a DAG by the programme [makefile2DAG](https://github.com/lindenb/makefile2graph), which uses another tool, graphviz.\n",
    "\n",
    "![Directed acyclic graph of make example](https://github.com/aeturrell/coding-for-economists/raw/main/img/makefile_example.png)\n",
    "\n",
    "Another good feature is that Make doesn't care *what* programme you're using: pdflatex, python, R, whatever, *as long as you can call it from the command line*. Which is basically all programmes. So it's very flexible too.\n",
    "\n",
    "One of the less nice features of Make is that it's quite complex, and it doesn't play that nicely with Windows. There is a lot extra that you can do with it, and with the complexity of your makefile, that goes beyond what we saw in the simple example above‚Äîthough, as noted, the [Turing's tutorial](https://the-turing-way.netlify.app/reproducible-research/make/make-examples.html) is a great place to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snakemake for Reproducible Analytical Pipelines\n",
    "\n",
    "[Snakemake](https://snakemake.readthedocs.io/en/stable/index.html) is a popular tool that shares a lot of similarities with Make but, instead of a Makefile, you write instructions in an arguably simpler format, yaml. Despite being very much geared around the Python ecosystem (and especially biological and genetic applications), it‚Äîlike Make‚Äîcan be used to automate any command that you would issue in your terminal.\n",
    "\n",
    "Here's an example (from biology) of a simple **snakemake** file, which would be saved as \"Snakefile\":\n",
    "\n",
    "```text\n",
    "rule bwa_map:\n",
    "    input:\n",
    "        \"data/genome.fa\",\n",
    "        \"data/samples/{sample}.fastq\"\n",
    "    output:\n",
    "        \"mapped_reads/{sample}.bam\"\n",
    "    shell:\n",
    "        \"bwa mem {input} | samtools view -Sb - > {output}\"\n",
    "```\n",
    "\n",
    "The rule here is \"bwa_map\", and there are two input datasets. One output dataset is created. The terminal command to run the operation as listed within the Snakefile is `bwa` pluss options and flags. Note that `{sample}`, `{input}`, and `{output}` are wildcards.\n",
    "\n",
    "In this case, example `snakemake mapped_reads/{A,B}.bam` on the command line in the same directory as the Snakefile would execute the `bwa_map` command in order to create both \"mapped_reads/A.bam\" and \"mapped_reads/B.bam\". The curly brackets make use of the fact that the command line addresses everything in curly brackets, and so address both the \"A\" and \"B\" cases.\n",
    "\n",
    "As with Make, the DAG defined by what you set out in your Snakefile can be turned into a plot using **graphviz**.\n",
    "\n",
    "This is just a very short introduction to Snakemake‚Äîyou can find more information in the [documentation](https://snakemake.readthedocs.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploomber for Reproducible Analytical Pipelines\n",
    "\n",
    "If you've just read the previous section on Make, you're probably thinking \"argh, why does this have to be so complicated!\". Well, happily, there are tools that aim to make defining a reproducible analytical pipeline *far* easier. [Ploomber](https://ploomber.readthedocs.io/) is one such tool that we'll look at in this section. It supports Python and R.\n",
    "\n",
    "```{note}\n",
    "If you wish to use Ploomber to create plots of your DAG, you will need to install Ploomber's optional dependencies, including [pygraphviz](https://pygraphviz.github.io/) and non-Python library [graphviz](https://www.graphviz.org). To install the latter on Mac, you can use homebrew to run `brew install graphviz` on the command line, while `sudo apt install graphviz` works on some flavours of Linux. You can download Windows installers and find more information about this [here](https://www.graphviz.org/download/).\n",
    "```\n",
    "\n",
    "Ploomber effectively does what Make does, but in a different way. One of its nice features is that it can tell when a pipeline was last executed and‚Äîif you make a change‚Äîit will only re-run scripts that are *downstream* of that change. It infers your DAG by having you add tags to your Python or R scripts that say what script they are upstream or downstream of (and any outputs they might create), and by having you write a yaml file with more info on the execution products. The three key commands are:\n",
    "- `ploomber plot`, which shows the DAG that you've defined;\n",
    "- `ploomber status`, which shows when each step was last executed; and\n",
    "- `ploomber build`, which (like `make`) runs the DAG.\n",
    "\n",
    "The \"first pipeline\" [Ploomber tutorial](https://ploomber.readthedocs.io/en/latest/get-started/spec-api-python.html) is a good place to start if you are interested in using this tool for RAPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducible Python Environments\n",
    "\n",
    "So, you've created your project's code in a modular way, you've used a sensible structure for your files, *and* you've set everything up as a reproducible analytical pipeline (or RAP). But now imagine a scenario where your computer breaks, and you need buy a new one, load it up, and try to run the code, or perhaps your colleague or co-author wants to run the code. What do you do!?\n",
    "\n",
    "For code to be truly reproducible, you, your colleagues, and your co-authors (and possibly reviewers too) need to be able to do reproduce the exact set of packages and version of programming languages that you used to generate your results.\n",
    "\n",
    "Here we hit upon another important principle of reproducible analysis: *you should use a clean, dedicated Python environment and set of packages for each project that you do.* This means that only packages that are necessary for your RAP need be installed and included, and makes it easier to isolate what others will need to re-run that project's scripts.\n",
    "\n",
    "Fortunately, Python makes this possible because it allows *virtual environments*: that is, you do not need to just have one, monolithic installation of Python on your computer but, instead, have many apparently different versions, each one dedicated to a particular project. And you can do it without the pain of actually installing many different versions of Python.\n",
    "\n",
    "It may be easier to illustrate with an example. Let's say you're using Python 3.8, statsmodels, and pandas for one project, project A. And, for project B, you're using Python 3.8 with numpy and scikit-learn. Best practice would be to have two separate virtual Python environments: environment A, with everything needed for project A, and environment B, with everything needed for project B. This way, when you \"hand over\" project A to a colleague, what they need is clear‚Äîand even specified as *part* of the documents contained within the project. Better still, you can specify the version of Python too.\n",
    "\n",
    "There are two ways one could create reproducible Python environments that we're going to look at in this section: poetry and Anaconda. *You are strongly recommended to use Poetry for full reproducibility*. Anaconda environments can get you most, but not all, of the way there and are an excellent back-up option.\n",
    "\n",
    "Before we get into the details though, a word of warning: even if you install the same packages and the same version of a programming language, you may still get differences on different operating systems (or other differences in computers, like hardware). This will be explored later on in this chapter.\n",
    "\n",
    "### Reproducible Environments with Poetry\n",
    "\n",
    "[Poetry](https://python-poetry.org/) is a relatively new but really quite amazing command line tool that performs several important tasks:\n",
    "- automatically creates virtual environments on a per project basis\n",
    "- manages Python package dependencies, and logs them automatically\n",
    "- manages Python package versions, and logs them automatically\n",
    "- installs all needed dependencies and packages from previously auto-generated (by Poetry) lists of versions\n",
    "- (advanced) builds Python packages\n",
    "- (advanced) uploads Python packages to PyPI where they can be installed (ie make your project pip installable)\n",
    "\n",
    "A pre-requisite for poetry is an installation of the version of Python that you plan to use for your project.\n",
    "\n",
    "To use poetry, install it, and navigate to your project folder within the command line. Then run `poetry init`. This will create a (human-readable) `pyproject.toml` file that lists the packages needed for your project. It will also create a virtual Python environment just for your project.\n",
    "\n",
    "To add and install packages, the command is `poetry add package-name`. As you add packages, you will see that `pyproject.toml` automatically becomes populated with what you installed. Here's an excerpt of the `pyproject.toml` file from the Python [skimpy](https://github.com/aeturrell/skimpy) package:\n",
    "\n",
    "```text\n",
    "[tool.poetry.dependencies]\n",
    "python = \">=3.7.1,<4.0.0\"\n",
    "click = \"^8.0.1\"\n",
    "rich = \"^10.9.0\"\n",
    "pandas = \"^1.3.2\"\n",
    "Pygments = \"^2.10.0\"\n",
    "typeguard = \"^2.12.1\"\n",
    "```\n",
    "\n",
    "The first line here says that this runs on Python 3.7.1 to any other Python version less than 4 (which doesn't yet exist). The next says that you should use at least verion 8.0.1 of the click package.\n",
    "\n",
    "A second file, `poetry.lock`, is also created. This \"locks\" down the dependencies of your packages. It's also human-readable, just not very compelling. Visual Studio Code will happily open both types of file.\n",
    "\n",
    "Where poetry really shines is when you send someone else the project. They receive all the files for your project, including the automatically generated `pyproject.toml` and `poetry.lock` files. Then, all they need is to have the right version of Python installed, navigate to the project folder in the command line, and enter `poetry install`. This will install all of the packages needed to run the code!\n",
    "\n",
    "Note that, if you want to use the virtual environment to run scripts or tools installed using `poetry add`, you will need to use `poetry run command-name` rather than the usual `command-name`. For example, if you have a script `analysis.py`, you would call it with `poetry run Python analysis.py`. You can also open a command line within the virtual environment using `poetry shell`.\n",
    "\n",
    "Although poetry is geared towards developers who are creating Python packages, it's fast becoming the best go-to option for reproducible Python environments. When using poetry for reproducible research, you're likely to want to pin to specific versions of all packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-reproducible Environments with Anaconda\n",
    "\n",
    "Anaconda is a package and environment manager that supports Python and R. It's very much geared toward data science, and has been around for a while. (This book uses it for its reproducible environment.)\n",
    "\n",
    "Note that Anaconda provides pre-built package binaries. This is especially useful on some systems where compiling packages is a nuisance (yes, it's Windows being difficult again). For this reason, and some others to do with guarantees around package quality, Anaconda is a popular choice for corporate coding setups.\n",
    "\n",
    "However, that doesn't mean it's a good choice. It has some serious failings when it comes to reproducibility, which we'll get to.\n",
    "\n",
    "Before we talk about reproducibility, we're going to look at creating conda environments from a file. While creating environments from a file isn't sufficient alone for reproducibility, it is necessary, and it's useful to know about in any case.\n",
    "\n",
    "**Anaconda and Python environments**\n",
    "\n",
    "This section outlines a really powerful and useful feature of Anaconda that doesn't quite get us to full reproducibility.\n",
    "\n",
    "Anaconda has an option for creating environments from a file. It uses an `environment.yml` file to do this. A yaml file is just a good old text file with the extension `.yml` instead of `.txt` and a certain layout of items (human-readable). You can open and edit yaml files in Visual Studio Code. Here's the contents of a simple `environment.yml` file; the packages are pinned here, but you don't have to do this (you can just have `pandas` instead of `pandas=1.3.3` and conda will install the latest version):\n",
    "\n",
    "```text\n",
    "name: codeforecon\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pip=21.2.4\n",
    "  - pandas=1.3.3\n",
    "  - pip:\n",
    "    - plotnine=0.8.0\n",
    "```\n",
    "\n",
    "There's quite a bit going on here so let's break it down. First, the version of Python. This is pinned to 3.8 by the line `python=3.8`. Second, the \"channels\" are where packages can be picked up from. The \"defaults\" channel provides the usual conda ones. \"conda-forge\" is a community-led effort that holds a superset of other packages and, by including it, you're saying you're happy for packages to come via this route too. Then the \"dependencies\" section lists the actual dependencies. These are what you'd get alternatively by running `conda install pandas=1.3.3`, etc., on the command line (assuming you had already installed Anaconda and put it on your computer's PATH).\n",
    "\n",
    "There are so many python packages out there that not all of them are available via conda or conda-forge, and sometimes the packages available on these channels are not the latest versions. So we also include `pip` as a dependency and then go on to say \"install some dependencies via `pip`\", that is install these as if a person had typed `pip install packagename=version` on the command line. Remember, pip installing a package pulls it down from PyPI, which is the biggest and most widely used Python package store.\n",
    "\n",
    "Once you have your `environment.yml` file, you can install it on the command line with:\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "It can take some time to create a new environment as Anaconda tries to resolve all possible clashes and dependencies in the packages you asked for (it takes longer the more packages you have). To use your newly created environment, it's\n",
    "\n",
    "```bash\n",
    "conda activate codeforecon\n",
    "```\n",
    "\n",
    "on the command line, where `codeforecon` what be whatever name you put at the start of your `environment.yml` file. If everything has worked you should see\n",
    "\n",
    "```bash\n",
    "(codeforecon) your-computer:your-folder your-name$\n",
    "```\n",
    "\n",
    "on the command line, if you're using bash, though this book recommends zsh. Now, every time you run a command, it will be in this Python environment. You can deactivate the environment with `conda deactivate` to go back to the base environment or `conda activate other-environment` (with the name of a different existing environment) to switch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semi-reproducible Anaconda Python environments**\n",
    "\n",
    "Now, having delved into how Anaconda environments work, let's think about how to make them reproducible. What we did above, pinning the version of *Python* and the packages is not going to cut the mustard. Conda environment.yml files are very useful for defining desired environments but there are times, as here, when we want to be able to *exactly* reproduce an environment‚Äîincluding the dependencies of the dependencies.\n",
    "\n",
    "Just as with poetry, we're going to end up with a main file listing the packages and a lock file with all the dependencies.\n",
    "\n",
    "First, we'll want to export our current environment to a file. Doing this from the command line, rather than writing the file out, ensures we capture what we have rather than just what we think we have! The command to create an `environment.yml` file is:\n",
    "\n",
    "```bash\n",
    "conda env export --from-history\n",
    "```\n",
    "\n",
    "Unfortunately, this does *not* track anything install with pip though!! That's a big failing of this approach. The other reason why it's not sufficient for reproducibility is that it doesn't track dependencies; for that we need something else.\n",
    "\n",
    "To close the final mile, we need an extra tool called [conda-lock](https://github.com/conda-incubator/conda-lock). If you're already using a conda environment with a particular set of packages, you can send those packages to lock files using\n",
    "\n",
    "```bash\n",
    "conda-lock -f environment.yml -p osx-64 -p linux-64\n",
    "```\n",
    "\n",
    "to create lockfiles for Mac OSX and Linux from your environment file. Conda packages are different on different hardware, so a different lockfile is needed for each operating system.\n",
    "\n",
    "There's a big drawback to doing this with Anaconda versus Poetry, and it's that while conda-lock takes care of anything you installed with `conda install`, it doesn't add anything to the \"lock\" file for the dependencies of packages that you installed with `pip install`. There's no good way around this, and you just have to hope that the pip dependencies are the ones you got the first time you used the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducible Computational Environments\n",
    "\n",
    "\"But it works on my computer!\" Ever heard that? The tools in the previous section are fantastic for reproducibility but only up to a point. The hard truth is that code packages prepared for different operating systems can have differences, most often if the package contains C or other compiled language code.\n",
    "\n",
    "For example, because it's hard to compile code on the fly in Windows, there are pre-compiled versions of popular packages available, either through Anaconda or just on the [websites of enthusiasts](https://www.lfd.uci.edu/~gohlke/pythonlibs/). The way Windows compiles code is different to Mac, which is different to Linux, and so on.\n",
    "\n",
    "So to be *really* sure that your project will run exactly the same on another system, you must not only reproduce the packages and their dependencies but also the computational environment they were run in.\n",
    "\n",
    "By far the most popular tool for creating virtual operating systems that you can then work in is called [Docker](https://www.docker.com/). Docker containers are a \"standard unit\" of software that packages up code and all its dependencies so a project runs the same regardless of what your computing environment looks like. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries (eg the operating system) and settings. Docker essentially puts a mini standardised operating system on top of whichever one you're using.\n",
    "\n",
    "To run Docker, you will need to [install it](https://www.docker.com/products/docker-desktop) but be warned‚Äîit's quite demanding on your system when it's running.\n",
    "\n",
    "Docker images are defined by \"dockerfiles\" and, as we've seen in the rest of this chapter, this is going to mean a plain text file with some weird formatting! Once a dockerfile is written it can then be built into an image (a time-consuming process), and finally run on your computer.\n",
    "\n",
    "How do you create a docker file (the text based version of your repo)? You can create a dockerfile automatically from a github repository using the [repo2docker](https://github.com/jupyterhub/repo2docker) tool.\n",
    "\n",
    "Another option is to create a Dockerfile specific to your project. Here's a simple example of a Dockerfile that loads a particular version of Python on a particular version of a Linux operating system:\n",
    "\n",
    "```text\n",
    "# Load the Debian Linux operating system with a specific version of Python installed\n",
    "FROM python:3.8.12-buster\n",
    "\n",
    "# Update package list and install some key libraries for compiling code\n",
    "RUN apt-get update && apt-get install -y gcc libffi-dev g++ libssl-dev openssl\n",
    "\n",
    "# Ensure \"python\" command points to this version of Python\n",
    "RUN update-alternatives --install /usr/bin/python python /usr/local/bin/python3.8 12\n",
    "RUN update-alternatives --install /usr/bin/python3 python3 /usr/local/bin/python3.8 12\n",
    "\n",
    "# ensure local python is preferred over any built-in python\n",
    "ENV PATH /usr/local/bin:$PATH\n",
    "\n",
    "# set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "```\n",
    "\n",
    "Of course, there's a Visual Studio Code extension for Dockerfiles.\n",
    "\n",
    "Note that for each instruction or command from the Dockerfile, the Docker builder generates an image layer and stacks it upon the previous ones. To make any rebuilding of your dockerfile less painful, you can ensure that only the later lines change, insofar as is possible.\n",
    "\n",
    "To build this docker image, the command line command to run in the relevant directory is\n",
    "\n",
    "```bash\n",
    "docker build -t pythonbasic .\n",
    "```\n",
    "\n",
    "which will tag the built docker image with the name \"pythonbasic\". To run the created image as a container, it's `\n",
    "\n",
    "```bash\n",
    "docker run -t -d --name pybasicrun pythonbasic\n",
    "```\n",
    "\n",
    "where `--name` gives the running instance a name `pybasicrun`. And then to enter the docker container and use commands within it, the command to launch the bash shell is\n",
    "\n",
    "```bash\n",
    "docker exec -i -t pybasicrun /bin/bash\n",
    "```\n",
    "\n",
    "If you run this, you should see something like:\n",
    "\n",
    "```bash\n",
    "root@ec5c6d10ebc3:/app#\n",
    "```\n",
    "\n",
    "```{admonition} Exercise\n",
    "Create a Dockerfile from the above text, build it, run the container, and then jump into it. Now, inside the docker container, run `printf \"print('hello world')\" >> test.py` and then `python test.py`. Your \"hello world\" message should appear on the command line.\n",
    "```\n",
    "\n",
    "If you can see this, congratulations! You've just set up your first Python-enabled Docker container. To quit the Docker container use <kbd>ctrl</kbd> + <kbd>c</kbd>, <kbd>ctrl</kbd> + <kbd>d</kbd>.\n",
    "\n",
    "To stop the container that's running and remove it in one go, use `docker rm pybasicrun --force` once you've quit the running instance.\n",
    "\n",
    "**Developing using Visual Studio Code within a Docker container**\n",
    "\n",
    "Now, developing on the command line like this is no fun, clearly. You want access to all of the power of an IDE. Fortunately, Visual Studio Code is designed to be able to connect from your desktop computer to the *inside* of your Docker container and provide a development experience as if you were running Visual Studio Code as normal.\n",
    "\n",
    "You can find instructions and a LOT more detail [here](https://code.visualstudio.com/docs/remote/attach-container#_attach-to-a-docker-container), but the short version is that you:\n",
    "\n",
    "1. Install the VS Code remote: containers extension ([more info on that here](https://code.visualstudio.com/docs/remote/remote-overview))\n",
    "2. Click on the 'remote explorer' icon on the left-hand panel in Visual Studio Code.\n",
    "3. Select \"containers\" from the drop-down menu (not \"SSH Targets\" or \"Github Codespaces\")\n",
    "4. Right-click on the image you'd like to connect to (perhaps \"pythonbasic\" from the example above), and hit \"Attach to container\"\n",
    "\n",
    "You should see a new Visual Studio Code window pop up with a message, \"Starting Dev Container\". This may take a while on the first run, as VS Code installs what it needs to connect remotely. But, once it's done, you'll find that you can navigate your Docker container, open a terminal in it, and do a lot of what you would normally but *within* the Docker container.\n",
    "\n",
    "As an aside, you can do much the same trick with other VS Code remote options‚ÄîSSH or Github Codespaces, but don't worry if you don't know what these are.\n",
    "\n",
    "Also note that you could do your entire development in a Docker container from the start if you wished to truly ensure that everything was isolated and repeatable. For extremely advanced users creating software tools, this [article explains how you can create a \"development container\"](https://code.visualstudio.com/docs/remote/create-dev-container) for software development. For everyone else, a variation on what we've seen would be fine.\n",
    "\n",
    "```{admonition} Exercise\n",
    "Attach Visual Studio Code to a docker container and then use the VS Code window that is connected to your running image to create a new file called test.py. Write `print('hello world')` in that file, save it, and use the VS Code integrated terminal to run it within the docker container.\n",
    "```\n",
    "\n",
    "**A docker image with poetry and all packages specified**\n",
    "\n",
    "This chapter is supposed to be about reproducibility, but we've taken a real detour into Docker and operating systems, and remote development. But have no fear because our digressions have been equipping us to create a fully reproducible environment.\n",
    "\n",
    "What we need for a fully reproducible environment is the code, the language, the packages, the dependencies of the packages, and the operating system to be specified. We are now in a position to do all that. We'll even create the DAG and pipeline using Make and graphviz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Your Analysis For Prosperity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Good Practice\n",
    "\n",
    "There are a few other good programming practices that you can follow to enhance the long-term sustainability and reproducibility of your code, especially as your code base gets bigger or more complex. You may not have heard of these yet, and they will appear in other chapters, but they are listed here for completeness.\n",
    "\n",
    "- unit testing\n",
    "- error handling\n",
    "- documentation of functions\n",
    "- using a code style such as [black]() TODO\n",
    "- input data validation\n",
    "- logging\n",
    "- continuous integration\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "671f4d32165728098ed6607f79d86bfe6b725b450a30021a55936f1af379a247"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('codeforecon': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
